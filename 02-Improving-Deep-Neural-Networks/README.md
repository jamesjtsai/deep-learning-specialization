#### Overview: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

Week | Learning Objectives
------------ | -------------
1 | Recall that different types of initializations lead to different results. Recognize the importance of initialization in complex neural networks. Recognize the difference between train/dev/test sets. Diagnose the bias and variance issues in your model. Learn when and how to use regularization methods such as dropout or L2 regularization. Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them. Use gradient checking to verify the correctness of your backpropagation implementation.
2 | Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam. Use random minibatches to accelerate the convergence and improve the optimization. Know the benefits of learning rate decay and apply it to your optimization.
3 | Master the process of hyperparameter tuning.
